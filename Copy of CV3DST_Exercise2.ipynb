{"cells":[{"cell_type":"markdown","metadata":{"id":"xABx3A-soZMM"},"source":["# Computer Vision III: Detection, Segmentation and Tracking (CV3DST) GNN Exercise\n","\n","In this exercise, we will first develop an extension of the ReID-based tracker we created in the previous exercise that will make it more robust to occlusions by allowing it to recover from missed detections. \n","\n","We will then implement a Message Passing Network from scratch, and we will use to build a model that will learn to combine position information and reid features to directly predict associations between past tracks and detections. We will use this model to create robust tracker. \n","\n","Your tasks are the following:\n","- Adapt the track management scheme of our ReIDTracker allow it to recover from missed detections.\n","- Implement a Message Passing Network from scratch to operate on bipartite graphs\n","- Implement the pairwise feature  computation to obtain features for our Message Passing Network\n","- Train the Message Passing Network and improve your tracker's IDF1 score\n","\n","\n","## Setup\n","\n","### Download and extract project data to your Google Drive\n","\n","1.   **Required**: Please follow all instructions of exercise 0 before running this notebook.\n","2.   Save this notebook to your Google Drive by clicking `Save a copy in Drive` from the `File` menu.\n","3.   Download [this](https://vision.in.tum.de/webshare/u/cetintas/cv3dst_gnn_exercise.zip) zip file to your desktop, extract it and upload it into the `Colab Notebooks` folder in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"DWELy8_1pDm9"},"source":["#### Connect the notebook to your Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QY4sb4bP7Wi8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657262954629,"user_tz":-120,"elapsed":22750,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"5e2598bd-eae5-40a4-bd6c-7bb0a5cb9408"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CDNCYKuF0VD"},"outputs":[],"source":["root_dir = \"gdrive/My Drive/Colab Notebooks/cv3dst_exercise/\"\n","gnn_root_dir = \"gdrive/My Drive/Colab Notebooks/cv3dst_gnn_exercise/\""]},{"cell_type":"code","source":["#!unzip \"gdrive/My Drive/Colab Notebooks/cv3dst_gnn_exercise.zip\" -d \"gdrive/My Drive/Colab Notebooks/cv3dst_gnn_exercise\""],"metadata":{"id":"EQa9Xt02DPH1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wme28GdFAFY"},"source":["The `root_dir` path points to the directory and the content in your Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5IArCFxEz-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657262957689,"user_tz":-120,"elapsed":3064,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"f6459c27-54b4-4371-de01-137ffd3bb6c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["preprocessed_data_test_2.pth  preprocessed_data_train_2.pth\n"]}],"source":["!ls \"gdrive/My Drive/Colab Notebooks/cv3dst_gnn_exercise/data\""]},{"cell_type":"markdown","metadata":{"id":"LFYSSMiwpxSq"},"source":["#### Install and import Python libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRMsynpFU6gh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657262972713,"user_tz":-120,"elapsed":15028,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"99bc8eb3-994e-4f3b-d8c0-84ce01567632"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n","Collecting lap\n","  Downloading lap-0.4.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 37.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: lap\n","  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lap: filename=lap-0.4.0-cp37-cp37m-linux_x86_64.whl size=1590224 sha256=4d8d7d9f5c92d0dc32ee8302187d944820727d1327e9477fc97334b7f94d5420\n","  Stored in directory: /root/.cache/pip/wheels/b1/0b/e3/ef9daf1b5547b56389e42c80c3100f1e6479bf5fd00fd9d6ba\n","Successfully built lap\n","Installing collected packages: lap\n","Successfully installed lap-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip\n","  Downloading https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip\n","\u001b[K     \\ 148 kB 6.4 MB/s\n","\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.21.6)\n","Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.3.5)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.4.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.1->motmetrics==1.1.3) (1.15.0)\n","Building wheels for collected packages: motmetrics\n","  Building wheel for motmetrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for motmetrics: filename=motmetrics-1.1.3-py3-none-any.whl size=134199 sha256=af2d7a8fdce1311c1458f944b53ef21db2505cbc708dee73edf9e82c2f8dd7d3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-20ns9kr8/wheels/39/60/bf/90b1b02ff42db1bf7f2d2fa3eef2fe8bc46061182cf4ce7b37\n","Successfully built motmetrics\n","Installing collected packages: motmetrics\n","Successfully installed motmetrics-1.1.3\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","!pip install tqdm lap\n","!pip install https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGOohkAgo-hW","executionInfo":{"status":"ok","timestamp":1657262983029,"user_tz":-120,"elapsed":10323,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cac7e55b-58eb-4588-cb2e-1ddd09e67ea1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  if __name__ == '__main__':\n"]}],"source":["import os\n","import sys\n","sys.path.append(os.path.join(gnn_root_dir, 'src'))\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","from tqdm.autonotebook import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from tracker.data_track import MOT16Sequences\n","from tracker.tracker import Tracker, ReIDTracker\n","from tracker.utils import run_tracker, cosine_distance\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","import os.path as osp\n","\n","import motmetrics as mm\n","mm.lap.default_solver = 'lap'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcbme23UQ5p2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657262985145,"user_tz":-120,"elapsed":2120,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"f3f7188f-1e4d-40e3-9293-58b372ed74b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["MOT16-02  MOT16-04  MOT16-05  MOT16-09\tMOT16-10  MOT16-11  MOT16-13\n","MOT16-01  MOT16-03  MOT16-06  MOT16-07\tMOT16-08  MOT16-12  MOT16-14\n"]}],"source":["!ls \"gdrive/My Drive/Colab Notebooks/cv3dst_exercise/data/MOT16/train\"\n","!ls \"gdrive/My Drive/Colab Notebooks/cv3dst_exercise/data/MOT16/test\""]},{"cell_type":"markdown","metadata":{"id":"-Qubi7uE6EPd"},"source":["## Speed-Ups\n","In order to speed up training and inference runtimes, in this exercise we will be working with pre-computed detections and ReID embeddings. We ran the object detector we provided in Exercise 0 and applied to all frames. We also computed reid embeddings for all boxes in every frame of the dataset so that they don't need to be computed every time you run your tracker. This yields over 10x speed improvements. You will not have to work directly with the resulting files, as we have internally adapted the boilerplate code to work with them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tESJ1I1n7ot1"},"outputs":[],"source":["train_db = torch.load(osp.join(gnn_root_dir, 'data/preprocessed_data_train_2.pth'))"]},{"cell_type":"markdown","metadata":{"id":"bqQQId537ot2"},"source":["## Exercise Part 0 - Assignment's 1 ReIDHungarianTracker\n","\n","We start by providing a sample solution of the ``ReIDTracker`` from Exercise 1.\n","It will serve as our baseline.\n","\n","\n","Recall that this tracker works by performing frame-to-frame bipartite matching between newly detected boxes and past tracks based on ReID distance. Whenever a past track cannot be matched, its killed. And whenever, a newly detected box cannot be match, it starts a new trajectory.\n","\n","**NOTE**: We have modified the ``compute_distance`` function in ``data_association`` from last week to include a thresshold on ReID distance (if ReID distance >0.1, matching is not possible). This is important to prevent our tracker from reusing tracks for very dissimilar objects.\n","\n","Results:\n","```\n","          IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 40.5% 57.5% 31.3% 52.2% 96.1%  62  12  38 12  390  8873 203  210 49.1% 0.090\n","MOT16-05 54.4% 64.4% 47.1% 68.8% 94.0% 133  54  67 12  305  2156 330  149 59.7% 0.142\n","MOT16-09 49.9% 61.7% 41.9% 66.3% 97.7%  26  13  12  1   82  1793  77   76 63.3% 0.082\n","MOT16-11 61.1% 67.4% 55.9% 80.2% 96.6%  75  44  24  7  266  1871 176   91 75.5% 0.083\n","OVERALL  49.6% 62.3% 41.2% 63.5% 96.1% 296 123 141 32 1043 14693 786  526 59.0% 0.097\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6ZQRUbo7ot2"},"outputs":[],"source":["_UNMATCHED_COST=255\n","class ReIDHungarianTracker(ReIDTracker):\n","    def data_association(self, boxes, scores, pred_features):  \n","        \"\"\"Refactored from previous implementation to split it onto distance computation and track management\"\"\"\n","        if self.tracks:\n","            track_boxes = torch.stack([t.box for t in self.tracks], axis=0)\n","            track_features = torch.stack([t.get_feature() for t in self.tracks], axis=0)\n","            \n","            distance = self.compute_distance_matrix(track_features, pred_features,\n","                                                    track_boxes, boxes, metric_fn=cosine_distance)\n","\n","            # Perform Hungarian matching.\n","            row_idx, col_idx = linear_assignment(distance)            \n","            self.update_tracks(row_idx, col_idx,distance, boxes, scores, pred_features)\n","\n","            \n","        else:\n","            # No tracks exist.\n","            self.add(boxes, scores, pred_features)\n","        \n","    def update_tracks(self, row_idx, col_idx, distance, boxes, scores, pred_features):\n","        \"\"\"Updates existing tracks and removes unmatched tracks.\n","           Reminder: If the costs are equal to _UNMATCHED_COST, it's not a \n","           match. \n","        \"\"\"\n","        track_ids = [t.id for t in self.tracks]\n","\n","        unmatched_track_ids = []\n","        seen_track_ids = []\n","        seen_box_idx = []\n","        for track_idx, box_idx in zip(row_idx, col_idx):\n","            costs = distance[track_idx, box_idx] \n","            internal_track_id = track_ids[track_idx]\n","            seen_track_ids.append(internal_track_id)\n","            if costs == _UNMATCHED_COST:\n","                unmatched_track_ids.append(internal_track_id)\n","            else:\n","                self.tracks[track_idx].box = boxes[box_idx]\n","                self.tracks[track_idx].add_feature(pred_features[box_idx])\n","                seen_box_idx.append(box_idx)\n","\n","        unseen_track_ids = set(track_ids) - set(seen_track_ids)\n","        unmatched_track_ids.extend(list(unseen_track_ids))\n","        self.tracks = [t for t in self.tracks\n","                       if t.id not in unmatched_track_ids]\n","\n","\n","        # Add new tracks.\n","        new_boxes_idx = set(range(len(boxes))) - set(seen_box_idx)\n","        new_boxes = [boxes[i] for i in new_boxes_idx]\n","        new_scores = [scores[i] for i in new_boxes_idx]\n","        new_features = [pred_features[i] for i in new_boxes_idx]\n","        self.add(new_boxes, new_scores, new_features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZ1vBklj7ot3"},"outputs":[],"source":["val_sequences = MOT16Sequences('MOT16-reid', root_dir = osp.join(root_dir, 'data/MOT16'), vis_threshold=0.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_z2Yl_ypm25","colab":{"base_uri":"https://localhost:8080/","height":669},"executionInfo":{"status":"ok","timestamp":1657263085373,"user_tz":-120,"elapsed":80072,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"2a8843fd-7b59-4044-a026-e9ccfe70fa7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 367\n","Runtime for MOT16-02: 6.1 s.\n","Tracking: MOT16-05\n","Tracks found: 464\n","Runtime for MOT16-05: 2.1 s.\n","Tracking: MOT16-09\n","Tracks found: 98\n","Runtime for MOT16-09: 1.5 s.\n","Tracking: MOT16-11\n","Tracks found: 292\n","Runtime for MOT16-11: 3.2 s.\n","Runtime for all sequences: 12.9 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 40.5% 57.5% 31.3% 52.2% 96.1%  62  12  38 12  390  8873 203  210 49.1% 0.090\n","MOT16-05 54.4% 64.4% 47.1% 68.8% 94.0% 133  54  67 12  305  2156 330  149 59.7% 0.142\n","MOT16-09 49.9% 61.7% 41.9% 66.3% 97.7%  26  13  12  1   82  1793  77   76 63.3% 0.082\n","MOT16-11 61.1% 67.4% 55.9% 80.2% 96.6%  75  44  24  7  266  1871 176   91 75.5% 0.083\n","OVERALL  49.6% 62.3% 41.2% 63.5% 96.1% 296 123 141 32 1043 14693 786  526 59.0% 0.097\n"]},{"output_type":"execute_result","data":{"text/plain":["              idf1       idp       idr    recall  precision  \\\n","MOT16-02  0.405105  0.575262  0.312631  0.522469   0.961378   \n","MOT16-05  0.544104  0.643506  0.471303  0.688304   0.939795   \n","MOT16-09  0.499161  0.617322  0.418967  0.663286   0.977310   \n","MOT16-11  0.611340  0.673988  0.559347  0.801717   0.966032   \n","OVERALL   0.495843  0.623022  0.411784  0.635038   0.960803   \n","\n","          num_unique_objects  mostly_tracked  partially_tracked  mostly_lost  \\\n","MOT16-02                  62              12                 38           12   \n","MOT16-05                 133              54                 67           12   \n","MOT16-09                  26              13                 12            1   \n","MOT16-11                  75              44                 24            7   \n","OVERALL                  296             123                141           32   \n","\n","          num_false_positives  num_misses  num_switches  num_fragmentations  \\\n","MOT16-02                  390        8873           203                 210   \n","MOT16-05                  305        2156           330                 149   \n","MOT16-09                   82        1793            77                  76   \n","MOT16-11                  266        1871           176                  91   \n","OVERALL                  1043       14693           786                 526   \n","\n","              mota      motp  \n","MOT16-02  0.490555  0.090340  \n","MOT16-05  0.596501  0.142002  \n","MOT16-09  0.633427  0.082336  \n","MOT16-11  0.754875  0.082736  \n","OVERALL   0.589607  0.096605  "],"text/html":["\n","  <div id=\"df-8757755d-a55e-4115-a566-6e77741cc4c0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf1</th>\n","      <th>idp</th>\n","      <th>idr</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>num_unique_objects</th>\n","      <th>mostly_tracked</th>\n","      <th>partially_tracked</th>\n","      <th>mostly_lost</th>\n","      <th>num_false_positives</th>\n","      <th>num_misses</th>\n","      <th>num_switches</th>\n","      <th>num_fragmentations</th>\n","      <th>mota</th>\n","      <th>motp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>MOT16-02</th>\n","      <td>0.405105</td>\n","      <td>0.575262</td>\n","      <td>0.312631</td>\n","      <td>0.522469</td>\n","      <td>0.961378</td>\n","      <td>62</td>\n","      <td>12</td>\n","      <td>38</td>\n","      <td>12</td>\n","      <td>390</td>\n","      <td>8873</td>\n","      <td>203</td>\n","      <td>210</td>\n","      <td>0.490555</td>\n","      <td>0.090340</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-05</th>\n","      <td>0.544104</td>\n","      <td>0.643506</td>\n","      <td>0.471303</td>\n","      <td>0.688304</td>\n","      <td>0.939795</td>\n","      <td>133</td>\n","      <td>54</td>\n","      <td>67</td>\n","      <td>12</td>\n","      <td>305</td>\n","      <td>2156</td>\n","      <td>330</td>\n","      <td>149</td>\n","      <td>0.596501</td>\n","      <td>0.142002</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-09</th>\n","      <td>0.499161</td>\n","      <td>0.617322</td>\n","      <td>0.418967</td>\n","      <td>0.663286</td>\n","      <td>0.977310</td>\n","      <td>26</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>82</td>\n","      <td>1793</td>\n","      <td>77</td>\n","      <td>76</td>\n","      <td>0.633427</td>\n","      <td>0.082336</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-11</th>\n","      <td>0.611340</td>\n","      <td>0.673988</td>\n","      <td>0.559347</td>\n","      <td>0.801717</td>\n","      <td>0.966032</td>\n","      <td>75</td>\n","      <td>44</td>\n","      <td>24</td>\n","      <td>7</td>\n","      <td>266</td>\n","      <td>1871</td>\n","      <td>176</td>\n","      <td>91</td>\n","      <td>0.754875</td>\n","      <td>0.082736</td>\n","    </tr>\n","    <tr>\n","      <th>OVERALL</th>\n","      <td>0.495843</td>\n","      <td>0.623022</td>\n","      <td>0.411784</td>\n","      <td>0.635038</td>\n","      <td>0.960803</td>\n","      <td>296</td>\n","      <td>123</td>\n","      <td>141</td>\n","      <td>32</td>\n","      <td>1043</td>\n","      <td>14693</td>\n","      <td>786</td>\n","      <td>526</td>\n","      <td>0.589607</td>\n","      <td>0.096605</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8757755d-a55e-4115-a566-6e77741cc4c0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8757755d-a55e-4115-a566-6e77741cc4c0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8757755d-a55e-4115-a566-6e77741cc4c0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["tracker = ReIDHungarianTracker(None)\n","run_tracker(val_sequences, db=train_db, tracker=tracker, output_dir=None)"]},{"cell_type":"markdown","metadata":{"id":"1NgkrWTy7ot4"},"source":["## Exercise Part I - Long-Term ReID Tracker\n"]},{"cell_type":"markdown","metadata":{"id":"9R8udP5I7ot4"},"source":["The tracker above has an obvious limitation: whenever a track cannot be matched with the detections of a given frame the track will be killed. This means that if our detector misses an object in a single frame (due to e.g. occlusion), we will not be able to recover that track, and we will start a new one. \n","\n","To fix this issue, we would like to allow our tracker to maintain tracks that are not matched during data association. We will refer to these tracks as **inactive**. During data association, we will try to match the detected boxes for the current frame to both tracks that are active (i.e. tracks that we were able to match in the previous frame) as well as those that are inactive. Therefore, if a detector misses an object in a frame and the object reappears after a few frames, we will still be able to match it to its corresponding track, instead of creating a new one.\n","\n","In order to adapt our tracker to have this behavior, we will use the `inactive` attribute from the `track` class (see `tracker/tracker.py`. This attribute will be assigned an integer indicating for how many frames a track has remained unmatched. Whenever we are able to match the track `t`, we will set `t.inactive=0` and, naturally, when tracks are initialized, the class constructor sets `inactive=0`. \n","\n","Your job is to maintain the `inactive` attribute of all tracks being kept by tracker so that its value represents the number of frames for which the track has been unmatched. Additionally, we introduce a `patience` parameter. Whenever a track has been inactive for more than `inactive` frames. it will need to be killed.\n","\n","Results should approximately be around:\n","\n","```\n","          IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 45.9% 65.1% 35.4% 52.2% 96.1%  62  12  37 13  390  8873 130  210 49.4% 0.090\n","MOT16-05 63.4% 75.0% 54.9% 68.8% 94.0% 133  54  67 12  305  2156 283  149 60.3% 0.142\n","MOT16-09 52.5% 64.9% 44.1% 66.3% 97.7%  26  13  12  1   82  1793  49   76 63.9% 0.083\n","MOT16-11 68.3% 75.3% 62.5% 80.2% 96.6%  75  44  24  7  266  1871 136   90 75.9% 0.083\n","OVERALL  55.7% 70.0% 46.2% 63.5% 96.1% 296 123 140 33 1043 14693 598  525 59.4% 0.097\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H84RSsmK7ot5"},"outputs":[],"source":["class LongTermReIDHungarianTracker(ReIDHungarianTracker):\n","    def __init__(self, patience, *args, **kwargs):\n","        \"\"\" Add a patience parameter\"\"\"\n","        self.patience=patience\n","        super().__init__(*args, **kwargs)\n","\n","    def update_results(self):\n","        \"\"\"Only store boxes for tracks that are active\"\"\"\n","        for t in self.tracks:\n","            if t.id not in self.results.keys():\n","                self.results[t.id] = {}\n","            if t.inactive == 0: # Only change\n","                self.results[t.id][self.im_index] = np.concatenate([t.box.cpu().numpy(), np.array([t.score])])\n","\n","        self.im_index += 1        \n","        \n","    def update_tracks(self, row_idx, col_idx, distance, boxes, scores, pred_features):\n","        track_ids = [t.id for t in self.tracks]\n","\n","        unmatched_track_ids = []\n","        seen_track_ids = []\n","        seen_box_idx = []\n","        for track_idx, box_idx in zip(row_idx, col_idx):\n","            costs = distance[track_idx, box_idx] \n","            internal_track_id = track_ids[track_idx]\n","            seen_track_ids.append(internal_track_id)\n","            if costs == _UNMATCHED_COST:\n","                unmatched_track_ids.append(internal_track_id)\n","\n","            else:\n","                self.tracks[track_idx].box = boxes[box_idx]\n","                self.tracks[track_idx].add_feature(pred_features[box_idx])\n","                \n","                # Note: the track is matched, therefore, inactive is set to 0\n","                self.tracks[track_idx].inactive=0\n","                seen_box_idx.append(box_idx)\n","                \n","\n","        unseen_track_ids = set(track_ids) - set(seen_track_ids)\n","        unmatched_track_ids.extend(list(unseen_track_ids))\n","        ##################\n","        ### TODO starts\n","        ##################\n","        \n","        # Update the `inactive` attribute for those tracks that have been \n","        # not been matched. kill those for which the inactive parameter \n","        # is > self.patience\n","\n","        records = []\n","        #iter over self.tracks\n","        for track in self.tracks:\n","          if track.id in unmatched_track_ids:\n","            track.inactive = track.inactive + 1\n","          if track.inactive > self.patience:\n","            records.append(track.id)\n","\n","        self.tracks = [x for x in self.tracks if x.id not in records] # <-- Needs to be updated\n","        \n","        ##################\n","        ### TODO ends\n","        ##################        \n","        \n","        new_boxes_idx = set(range(len(boxes))) - set(seen_box_idx)\n","        new_boxes = [boxes[i] for i in new_boxes_idx]\n","        new_scores = [scores[i] for i in new_boxes_idx]\n","        new_features = [pred_features[i] for i in new_boxes_idx]\n","        self.add(new_boxes, new_scores, new_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-COcYAu7ot6","colab":{"base_uri":"https://localhost:8080/","height":669},"executionInfo":{"status":"ok","timestamp":1657263150647,"user_tz":-120,"elapsed":65280,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"f3838b5a-87fc-46a6-b5a6-6781de104242"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 207\n","Runtime for MOT16-02: 6.7 s.\n","Tracking: MOT16-05\n","Tracks found: 355\n","Runtime for MOT16-05: 2.5 s.\n","Tracking: MOT16-09\n","Tracks found: 65\n","Runtime for MOT16-09: 1.8 s.\n","Tracking: MOT16-11\n","Tracks found: 217\n","Runtime for MOT16-11: 3.3 s.\n","Runtime for all sequences: 14.3 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 45.9% 65.1% 35.4% 52.2% 96.1%  62  12  37 13  390  8873 130  210 49.4% 0.090\n","MOT16-05 63.4% 75.0% 54.9% 68.8% 94.0% 133  54  67 12  305  2156 283  149 60.3% 0.142\n","MOT16-09 52.5% 64.9% 44.1% 66.3% 97.7%  26  13  12  1   82  1793  49   76 63.9% 0.083\n","MOT16-11 68.3% 75.3% 62.5% 80.2% 96.6%  75  44  24  7  266  1871 136   90 75.9% 0.083\n","OVERALL  55.7% 70.0% 46.2% 63.5% 96.1% 296 123 140 33 1043 14693 598  525 59.4% 0.097\n"]},{"output_type":"execute_result","data":{"text/plain":["              idf1       idp       idr    recall  precision  \\\n","MOT16-02  0.458524  0.651119  0.353856  0.522469   0.961378   \n","MOT16-05  0.633898  0.749704  0.549082  0.688304   0.939795   \n","MOT16-09  0.525115  0.649419  0.440751  0.663286   0.977310   \n","MOT16-11  0.683269  0.753288  0.625159  0.801717   0.966032   \n","OVERALL   0.556888  0.699726  0.462480  0.635038   0.960803   \n","\n","          num_unique_objects  mostly_tracked  partially_tracked  mostly_lost  \\\n","MOT16-02                  62              12                 37           13   \n","MOT16-05                 133              54                 67           12   \n","MOT16-09                  26              13                 12            1   \n","MOT16-11                  75              44                 24            7   \n","OVERALL                  296             123                140           33   \n","\n","          num_false_positives  num_misses  num_switches  num_fragmentations  \\\n","MOT16-02                  390        8873           130                 210   \n","MOT16-05                  305        2156           283                 149   \n","MOT16-09                   82        1793            49                  76   \n","MOT16-11                  266        1871           136                  90   \n","OVERALL                  1043       14693           598                 525   \n","\n","              mota      motp  \n","MOT16-02  0.494484  0.090339  \n","MOT16-05  0.603296  0.141942  \n","MOT16-09  0.638685  0.082643  \n","MOT16-11  0.759114  0.082754  \n","OVERALL   0.594277  0.096641  "],"text/html":["\n","  <div id=\"df-e7c5b32f-b821-47c6-9f8f-e29bafd5f44c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idf1</th>\n","      <th>idp</th>\n","      <th>idr</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>num_unique_objects</th>\n","      <th>mostly_tracked</th>\n","      <th>partially_tracked</th>\n","      <th>mostly_lost</th>\n","      <th>num_false_positives</th>\n","      <th>num_misses</th>\n","      <th>num_switches</th>\n","      <th>num_fragmentations</th>\n","      <th>mota</th>\n","      <th>motp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>MOT16-02</th>\n","      <td>0.458524</td>\n","      <td>0.651119</td>\n","      <td>0.353856</td>\n","      <td>0.522469</td>\n","      <td>0.961378</td>\n","      <td>62</td>\n","      <td>12</td>\n","      <td>37</td>\n","      <td>13</td>\n","      <td>390</td>\n","      <td>8873</td>\n","      <td>130</td>\n","      <td>210</td>\n","      <td>0.494484</td>\n","      <td>0.090339</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-05</th>\n","      <td>0.633898</td>\n","      <td>0.749704</td>\n","      <td>0.549082</td>\n","      <td>0.688304</td>\n","      <td>0.939795</td>\n","      <td>133</td>\n","      <td>54</td>\n","      <td>67</td>\n","      <td>12</td>\n","      <td>305</td>\n","      <td>2156</td>\n","      <td>283</td>\n","      <td>149</td>\n","      <td>0.603296</td>\n","      <td>0.141942</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-09</th>\n","      <td>0.525115</td>\n","      <td>0.649419</td>\n","      <td>0.440751</td>\n","      <td>0.663286</td>\n","      <td>0.977310</td>\n","      <td>26</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>82</td>\n","      <td>1793</td>\n","      <td>49</td>\n","      <td>76</td>\n","      <td>0.638685</td>\n","      <td>0.082643</td>\n","    </tr>\n","    <tr>\n","      <th>MOT16-11</th>\n","      <td>0.683269</td>\n","      <td>0.753288</td>\n","      <td>0.625159</td>\n","      <td>0.801717</td>\n","      <td>0.966032</td>\n","      <td>75</td>\n","      <td>44</td>\n","      <td>24</td>\n","      <td>7</td>\n","      <td>266</td>\n","      <td>1871</td>\n","      <td>136</td>\n","      <td>90</td>\n","      <td>0.759114</td>\n","      <td>0.082754</td>\n","    </tr>\n","    <tr>\n","      <th>OVERALL</th>\n","      <td>0.556888</td>\n","      <td>0.699726</td>\n","      <td>0.462480</td>\n","      <td>0.635038</td>\n","      <td>0.960803</td>\n","      <td>296</td>\n","      <td>123</td>\n","      <td>140</td>\n","      <td>33</td>\n","      <td>1043</td>\n","      <td>14693</td>\n","      <td>598</td>\n","      <td>525</td>\n","      <td>0.594277</td>\n","      <td>0.096641</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7c5b32f-b821-47c6-9f8f-e29bafd5f44c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e7c5b32f-b821-47c6-9f8f-e29bafd5f44c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e7c5b32f-b821-47c6-9f8f-e29bafd5f44c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":13}],"source":["tracker = LongTermReIDHungarianTracker(patience=20, obj_detect=None)\n","run_tracker(val_sequences, db=train_db, tracker=tracker, output_dir=None)"]},{"cell_type":"markdown","metadata":{"id":"CBeSJrY87ot7"},"source":["## Exercise Part II - Building a tracker based on Neural Message Passing\n","\n","Our ``LongTermReIDHungarianTracker`` is still limited when compared to current modern trackers. \n","\n","Firstly, it relies solely on appearance to predict similarity scores between objectes. This can be problematic whenever appearance alone may not discriminative, and it'd be best to also take into account object position and size attributes. Secondly, our tracker can only account for pairwise similarities among objects. Ideally, we would like it to also consider higher-order information.\n","\n","To address these limitations. We will now build a tracker that will combine both apperance and position information with a Message Passing Neural Network, inspired by the approach presented in [Learning a Neural Solver for Multiple Object Tracking, CVPR 2020](https://arxiv.org/abs/1912.07515)\n","\n","The overall idea will be to build, for every tracking step, a bipartite graph containing two sets of nodes: past tracks, and detections in the current frame. We will initialize node features with ReID embeddings, and edge features with relative position features and ReID distance. We will use an MPN to refine these edge embeddings. The learning task will be to classify the edge embeddings in this graph, which is equivalent to predicting the entries of our data association similarity matrix.\n"]},{"cell_type":"markdown","metadata":{"id":"eVbnOrlPqWRB"},"source":["### Building an MPN for Bipartite Graphs\n","\n","We will first build a Neural Message Passing layer based on the Graph Networks framework introduced in [Relational inductive biases, deep learning, and graph networks, arXiv 2020](https://arxiv.org/abs/1806.01261), as explained in the *A More General Framework* slides of [Lecture 5](https://www.moodle.tum.de/pluginfile.php/2928927/mod_resource/content/1/5.MOT2.pdf) (slides 70 to 75).\n","\n","We will be using a bipartite graph, i.e., we will have two sets of nodes $A$ (past tracks), and $B$ (detections), and our set of edges will be $A\\times B$. That is, we will connect every pair of past tracks and detections.\n","\n","We will have initial node features (i.e. reid embeddings) matrices: $X_A$ and $X_B$ and an initial edge features tensor $E$.\n","\n","$X_A$ and $X_B$ have shape $|A|\\times \\text{node\\_dim}$ and $|B|\\times \\text{node\\_dim}$, respectively.\n","\n","$E$ has shape $|A| \\times |B| \\times \\text{edge\\_dim}$. Its $(i, j)$ entry contains the edge features of node $i$ in $A$ and node $j$ in $B$.\n","\n","With the given layer, we will produce new node feature matrices $X_A'$ and $X_B'$ and edge features $E'$ with the same dimensions. \n","Please refer to the formulas in the slides and figure how to apply them in this setting.\n","\n","You are asked to implement both the node and edge update steps in the class below\n","\n","**NOTE 1**: Working with a bipartite graph allows us to vectorize all operations in the formulas in a straightforward manner (keep in mind that we store edge features in a matrix). Given a node in $A$, it is connected to all nodes in $B$.\n","\n","**NOTE 2**: You do not need to care about batching several graphs. This implementation will only work with a single graph at a time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4q2I6Luo7ot7"},"outputs":[],"source":["from torch import nn\n","\n","class BipartiteNeuralMessagePassingLayer(nn.Module):    \n","    def __init__(self, node_dim, edge_dim, dropout=0.):\n","        super().__init__()\n","\n","        edge_in_dim  = 2*node_dim + 2*edge_dim # 2*edge_dim since we always concatenate initial edge features\n","        self.edge_mlp = nn.Sequential(*[nn.Linear(edge_in_dim, edge_dim), nn.ReLU(), nn.Dropout(dropout), \n","                                    nn.Linear(edge_dim, edge_dim), nn.ReLU(), nn.Dropout(dropout)])\n","\n","        node_in_dim  = node_dim + edge_dim\n","        self.node_mlp = nn.Sequential(*[nn.Linear(node_in_dim, node_dim), nn.ReLU(), nn.Dropout(dropout),  \n","                                        nn.Linear(node_dim, node_dim), nn.ReLU(), nn.Dropout(dropout)])\n","\n","    def edge_update(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n","        \"\"\"\n","        Node-to-edge updates, as descibed in slide 71, lecture 5.\n","        Args:\n","            edge_embeds: torch.Tensor with shape (|A|, |B|, 2 x edge_dim) \n","            nodes_a_embeds: torch.Tensor with shape (|A|, node_dim)\n","            nodes_a_embeds: torch.Tensor with shape (|B|, node_dim)\n","            \n","        returns:\n","            updated_edge_feats = torch.Tensor with shape (|A|, |B|, edge_dim) \n","        \"\"\"\n","        \n","        n_nodes_a, n_nodes_b, _  = edge_embeds.shape\n","        \n","        ########################\n","        #### TODO starts\n","        ########################\n","        \n","        na = nodes_a_embeds.view(nodes_a_embeds.shape[0], 1, nodes_a_embeds.shape[1]).repeat(1, nodes_b_embeds.shape[0], 1)\n","        nb = nodes_b_embeds.view(1, nodes_b_embeds.shape[0], nodes_b_embeds.shape[1]).repeat(nodes_a_embeds.shape[0], 1, 1)\n","        edge_in = torch.cat((edge_embeds, na, nb), dim=2)\n","        \n","        ########################\n","        #### TODO ends\n","        ########################\n","        \n","        \n","        return self.edge_mlp(edge_in)\n","\n","    def node_update(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n","        \"\"\"\n","        Edge-to-node updates, as descibed in slide 75, lecture 5.\n","\n","        Args:\n","            edge_embeds: torch.Tensor with shape (|A|, |B|, edge_dim) \n","            nodes_a_embeds: torch.Tensor with shape (|A|, node_dim)\n","            nodes_b_embeds: torch.Tensor with shape (|B|, node_dim)\n","            \n","        returns:\n","            tuple(\n","                updated_nodes_a_embeds: torch.Tensor with shape (|A|, node_dim),\n","                updated_nodes_b_embeds: torch.Tensor with shape (|B|, node_dim)\n","                )\n","        \"\"\"\n","        \n","        ########################\n","        #### TODO starts\n","        ########################\n","        \n","        # NOTE: Use 'sum' as aggregation function\n","\n","        \n","        nodes_a_in = torch.cat((nodes_a_embeds, torch.sum(edge_embeds, dim=1)), dim=1)\n","        nodes_b_in = torch.cat((nodes_b_embeds, torch.sum(edge_embeds, dim=0)), dim=1)\n","\n","        ########################\n","        #### TODO ends\n","        ########################\n","\n","        nodes_a = self.node_mlp(nodes_a_in)\n","        nodes_b = self.node_mlp(nodes_b_in)\n","\n","        return nodes_a, nodes_b\n","\n","    def forward(self, edge_embeds, nodes_a_embeds, nodes_b_embeds):\n","        edge_embeds_latent = self.edge_update(edge_embeds, nodes_a_embeds, nodes_b_embeds)\n","        nodes_a_latent, nodes_b_latent = self.node_update(edge_embeds_latent, nodes_a_embeds, nodes_b_embeds)\n","\n","        return edge_embeds_latent, nodes_a_latent, nodes_b_latent"]},{"cell_type":"markdown","metadata":{"id":"F2sPoUo07ot8"},"source":["## Building the entire network to predict similarities\n","We now build the network that generates initial node and edge features, performs neural message passing, and classifies edges in order to produce the final costs that we will use for data association.\n","\n","You need to implement the method that computes the initial edge features. You can can follow [1] and, given a two bounding boxes $(x_i, y_i, w_i, h_i)$ and  $(x_j, y_j, w_j, h_j)$ and timestamps $t_i$ and $t_j$, compute an initial 5-dimensional edge feature vector as:\n","$$ E_(i, j) = \\left (\\frac{2(x_j - x_i)}{h_i + h_j}, \\frac{2(y_j - y_i)}{h_i + h_j}, \\log{\\frac{h_i}{h_j}}, \\log{\\frac{w_i}{w_j}}, t_j - t_i \\right )$$\n","\n","\n","Feel free to engineer your own features (e.g. use IoU, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nF7apF9C7ot8"},"outputs":[],"source":["from torch.nn import functional as F\n","class AssignmentSimilarityNet(nn.Module):\n","    def __init__(self, reid_network, node_dim, edge_dim, reid_dim, edges_in_dim, num_steps, dropout=0.):\n","        super().__init__()\n","        self.reid_network = reid_network\n","        self.graph_net = BipartiteNeuralMessagePassingLayer(node_dim=node_dim, edge_dim=edge_dim, dropout=dropout)\n","        self.num_steps = num_steps\n","        self.cnn_linear = nn.Linear(reid_dim, node_dim)\n","        self.edge_in_mlp = nn.Sequential(*[nn.Linear(edges_in_dim, edge_dim), nn.ReLU(), nn.Dropout(dropout), nn.Linear(edge_dim, edge_dim), nn.ReLU(),nn.Dropout(dropout)])\n","        self.classifier = nn.Sequential(*[nn.Linear(edge_dim, edge_dim), nn.ReLU(), nn.Linear(edge_dim, 1)])\n","        \n","    \n","    def compute_edge_feats(self, track_coords, current_coords, track_t, curr_t):    \n","        \"\"\"\n","        Computes initial edge feature tensor\n","\n","        Args:\n","            track_coords: track's frame box coordinates, given by top-left and bottom-right coordinates\n","                          torch.Tensor with shape (num_tracks, 4)\n","            current_coords: current frame box coordinates, given by top-left and bottom-right coordinates\n","                            has shape (num_boxes, 4)\n","                          \n","            track_t: track's timestamps, torch.Tensor with with shape (num_tracks, )\n","            curr_t: current frame's timestamps, torch.Tensor withwith shape (num_boxes,)        \n","            \n","        \n","        Returns:\n","            tensor with shape (num_trakcs, num_boxes, 5) containing pairwise\n","            position and time difference features \n","        \"\"\"\n","\n","        ########################\n","        #### TODO starts\n","        ########################\n","        \n","        # NOTE 1: we recommend you to use box centers to compute distances\n","        # in the x and y coordinates.\n","\n","        # NOTE 2: Check out the  code inside train_one_epoch function and \n","        # LongTrackTrainingDataset class a few cells below to debug this\n","        ###\n","\n","        edge_feats = torch.zeros(track_coords.shape[0], current_coords.shape[0], 5)\n","        for i in range(track_coords.shape[0]):\n","          for j in range(current_coords.shape[0]):\n","            wi = track_coords[i, 2] - track_coords[i, 0]\n","            wj = current_coords[j, 2] - current_coords[j, 0]\n","            hi = track_coords[i, 3] - track_coords[i, 1]\n","            hj = current_coords[j, 3] - current_coords[j, 1]\n","            xi = (track_coords[i, 0] + track_coords[i, 2]) / 2\n","            yi = (track_coords[i, 1] + track_coords[i, 3]) / 2\n","            xj = (current_coords[j, 0] + current_coords[j, 2]) / 2\n","            yj = (current_coords[j, 1] + current_coords[j, 3]) / 2\n","            edge_feats[i, j, 4] = curr_t[j] - track_t[i]\n","            edge_feats[i, j, 2] = torch.log(hi / hj)\n","            edge_feats[i, j, 3] = torch.log(wi / wj)\n","            edge_feats[i, j, 0] = 2 * (xj - xi) / (hj + hi)\n","            edge_feats[i, j, 1] = 2 * (yj - yi) / (hj + hi)\n","\n","        edge_feats = edge_feats.to(track_coords.device)\n","\n","\n","        ########################\n","        #### TODO ends\n","        ########################\n","\n","        return edge_feats # has shape (num_trakcs, num_boxes, 5)\n","\n","\n","    def forward(self, track_app, current_app, track_coords, current_coords, track_t, curr_t):\n","        \"\"\"\n","        Args:\n","            track_app: track's reid embeddings, torch.Tensor with shape (num_tracks, 512)\n","            current_app: current frame detections' reid embeddings, torch.Tensor with shape (num_boxes, 512)\n","            track_coords: track's frame box coordinates, given by top-left and bottom-right coordinates\n","                          torch.Tensor with shape (num_tracks, 4)\n","            current_coords: current frame box coordinates, given by top-left and bottom-right coordinates\n","                            has shape (num_boxes, 4)\n","                          \n","            track_t: track's timestamps, torch.Tensor with with shape (num_tracks, )\n","            curr_t: current frame's timestamps, torch.Tensor withwith shape (num_boxes,)\n","            \n","        Returns:\n","            classified edges: torch.Tensor with shape (num_steps, num_tracks, num_boxes),\n","                             containing at entry (step, i, j) the unnormalized probability that track i and \n","                             detection j are a match, according to the classifier at the given neural message passing step\n","        \"\"\"\n","        \n","        # Get initial edge embeddings to\n","        dist_reid = cosine_distance(track_app, current_app)\n","        pos_edge_feats = self.compute_edge_feats(track_coords, current_coords, track_t, curr_t)\n","        edge_feats = torch.cat((pos_edge_feats, dist_reid.unsqueeze(-1)), dim=-1)\n","        edge_embeds = self.edge_in_mlp(edge_feats)\n","        initial_edge_embeds = edge_embeds.clone()\n","\n","        # Get initial node embeddings, reduce dimensionality from 512 to node_dim\n","        track_embeds = F.relu(self.cnn_linear(track_app))\n","        curr_embeds =F.relu(self.cnn_linear(current_app))\n","\n","        classified_edges = []\n","        for _ in range(self.num_steps):\n","            edge_embeds = torch.cat((edge_embeds, initial_edge_embeds), dim=-1)            \n","            edge_embeds, track_embeds, curr_embeds = self.graph_net(edge_embeds=edge_embeds, \n","                                                                    nodes_a_embeds=track_embeds, \n","                                                                    nodes_b_embeds=curr_embeds)\n","\n","            classified_edges.append(self.classifier(edge_embeds))\n","\n","        return torch.stack(classified_edges).squeeze(-1)"]},{"cell_type":"markdown","metadata":{"id":"7Cdr6YDY7ot8"},"source":["## Putting everything together"]},{"cell_type":"markdown","metadata":{"id":"Xp3cC6j57ot9"},"source":["Finally, we incorporate our ``AssignmentSimilarityNet`` into our tracker. We can keep everything as in ``LongTermReIDHungarianTracker`` except for the distance computation, which is now directly obtained via a forward pass through AssignmentSimilarityNet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GA99XOv7ot9"},"outputs":[],"source":["_UNMATCHED_COST=255\n","class MPNTracker(LongTermReIDHungarianTracker):\n","    def __init__(self, assign_net, *args, **kwargs):\n","        self.assign_net = assign_net\n","        super().__init__(*args, **kwargs)\n","        \n","    def data_association(self, boxes, scores, pred_features):  \n","        if self.tracks:  \n","            track_boxes = torch.stack([t.box for t in self.tracks], axis=0).cuda()\n","            track_features = torch.stack([t.get_feature() for t in self.tracks], axis=0).cuda()\n","            \n","            # Hacky way to recover the timestamps of boxes and tracks\n","            curr_t = self.im_index * torch.ones((pred_features.shape[0],)).cuda()\n","            track_t = torch.as_tensor([self.im_index - t.inactive - 1 for t in self.tracks]).cuda()\n","\n","            ########################\n","            #### TODO starts\n","            ########################\n","            \n","            # Do a forward pass through self.assign_net to obtain our costs.\n","            # Note: self.assign_net will return unnormalized probabilities. \n","            # Make sure to apply the sigmoid function to them!\n","\n","\n","            fwd = self.assign_net.forward(track_features, pred_features.cuda(), track_boxes, boxes.cuda(), track_t, curr_t)\n","            pred_sim = torch.sigmoid(fwd).cpu().data.numpy()\n","\n","\n","            ########################\n","            #### TODO ends\n","            ########################\n","\n","            pred_sim = pred_sim[-1]  # Use predictions at last message passing step\n","            distance = (1- pred_sim) \n","            \n","            # Do not allow mataches when sim < 0.5, to avoid low-confident associations\n","            distance = np.where(pred_sim < 0.5, _UNMATCHED_COST, distance) \n","\n","            # Perform Hungarian matching.\n","            row_idx, col_idx = linear_assignment(distance)            \n","            self.update_tracks(row_idx, col_idx,distance, boxes, scores, pred_features)\n","\n","            \n","        else:\n","            # No tracks exist.\n","            self.add(boxes, scores, pred_features)"]},{"cell_type":"markdown","metadata":{"id":"cyzMUfYE7ot9"},"source":["## Training and evaluating our model\n","\n","We provide all boilerplate code for training our neural message passing based\n","tracker, as well as evaluating. \n","\n","Under the hood, we are sampling frames randomly from our training sequences, and then sampling boxes from past frames as past_tracks to generate our \n","training\n","data. Check out `LongTrackTrainingDataset` for details.\n","\n","We train the model with a weighted cross-entropy loss\n","to account for the class imbalance. Check out `train_one_epoch` if you're \n","interested.\n","\n","No need to write any code from your side here!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuYy1IMK7ot9"},"outputs":[],"source":["from gnn.dataset import LongTrackTrainingDataset\n","from torch.utils.data import DataLoader\n","from gnn.trainer import train_one_epoch\n","\n","MAX_PATIENCE = 20\n","MAX_EPOCHS = 15\n","EVAL_FREQ = 1\n","\n","\n","# Define our model, and init \n","assign_net = AssignmentSimilarityNet(reid_network=None, # Not needed since we work with precomputed features\n","                                     node_dim=32, \n","                                     edge_dim=64, \n","                                     reid_dim=512, \n","                                     edges_in_dim=6, \n","                                     num_steps=10).cuda()\n","\n","# We only keep two sequences for validation. You can\n","dataset = LongTrackTrainingDataset(dataset='MOT16-train_wo_val2', \n","                                   db=train_db, \n","                                   root_dir= osp.join(root_dir, 'data/MOT16'),\n","                                   max_past_frames = MAX_PATIENCE,\n","                                   vis_threshold=0.25)\n","\n","data_loader = DataLoader(dataset, batch_size=8, collate_fn = lambda x: x, \n","                         shuffle=True, num_workers=2, drop_last=True)\n","device = torch.device('cuda')\n","optimizer = torch.optim.Adam(assign_net.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5)"]},{"cell_type":"markdown","metadata":{"id":"FxiIUDGn2_Nq"},"source":["We only leave 2 sequences for validation in order to maximize \n","the amount of training data. For your convenience, here are the\n"," LongTermReIDTracker results on them. Your validation IDF1 scores should show an improvement over them.\n","```\n","\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 45.9% 65.1% 35.4% 52.2% 96.1%  62 12 37 13 390  8873 130  210 49.4% 0.090\n","MOT16-11 68.3% 75.3% 62.5% 80.2% 96.6%  75 44 24  7 266  1871 136   90 75.9% 0.083\n","OVERALL  54.3% 69.6% 44.5% 61.7% 96.3% 137 56 61 20 656 10744 266  300 58.4% 0.087\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V7tCJJ3V3Oaa"},"source":["Let's start training!\n","\n","Note that we have observed quite a lot of noise in validation scores among epochs and runs. This can be explained due to the small size of our training and\n","validation sets, that's why we perform early stopping to obtain the best performing model on validation. In addition, changing the experiment seed and/or relaunching the training might help in case you are suspecting that noise might be influencing your scores. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQHiwbVr7ot9","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657277047094,"user_tz":-120,"elapsed":13875523,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"75a0dd4d-469d-4f61-8090-e1b5c9b7d6a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------- EPOCH  1 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:44,  1.61s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.645. Accuracy: 0.949. Recall: 0.734. Precision: 0.550\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:21,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.093. Accuracy: 0.993. Recall: 0.990. Precision: 0.908\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:06,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.037. Accuracy: 0.998. Recall: 0.995. Precision: 0.974\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:52,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.020. Accuracy: 0.999. Recall: 0.998. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["476it [13:02,  1.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 80\n","Runtime for MOT16-02: 89.2 s.\n","Tracking: MOT16-11\n","Tracks found: 74\n","Runtime for MOT16-11: 42.6 s.\n","Runtime for all sequences: 131.7 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 45.2% 64.2% 34.9% 52.2% 96.1%  62 11 38 13 390  8873 103  221 49.6% 0.095\n","MOT16-11 64.0% 70.6% 58.6% 80.2% 96.6%  75 44 24  7 266  1871  39   90 76.9% 0.083\n","OVERALL  52.3% 67.0% 42.9% 61.7% 96.3% 137 55 62 20 656 10744 142  311 58.8% 0.090\n","-------- EPOCH  2 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:46,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.017. Accuracy: 0.999. Recall: 0.998. Precision: 0.988\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:25,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.021. Accuracy: 0.999. Recall: 0.999. Precision: 0.985\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:19,  1.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.015. Accuracy: 0.999. Recall: 0.998. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["400it [11:01,  1.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.013. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["476it [13:03,  1.65s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 85\n","Runtime for MOT16-02: 91.8 s.\n","Tracking: MOT16-11\n","Tracks found: 80\n","Runtime for MOT16-11: 43.4 s.\n","Runtime for all sequences: 135.2 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.0% 66.7% 36.3% 52.2% 96.1%  62 11 38 13 390  8873  98  222 49.6% 0.095\n","MOT16-11 71.4% 78.7% 65.3% 80.2% 96.6%  75 44 24  7 266  1871  32   90 77.0% 0.083\n","OVERALL  56.2% 72.0% 46.1% 61.7% 96.3% 137 55 62 20 656 10744 130  312 58.8% 0.090\n","-------- EPOCH  3 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:41,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.013. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:21,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.016. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:07,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.014. Accuracy: 0.999. Recall: 0.998. Precision: 0.987\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:54,  1.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.013. Accuracy: 0.999. Recall: 0.998. Precision: 0.987\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:57,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 91\n","Runtime for MOT16-02: 88.8 s.\n","Tracking: MOT16-11\n","Tracks found: 81\n","Runtime for MOT16-11: 42.1 s.\n","Runtime for all sequences: 130.9 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.0% 66.7% 36.2% 52.2% 96.1%  62 11 38 13 390  8873 109  217 49.6% 0.095\n","MOT16-11 69.4% 76.5% 63.5% 80.2% 96.6%  75 44 24  7 266  1871  35   90 77.0% 0.083\n","OVERALL  55.4% 71.0% 45.4% 61.7% 96.3% 137 55 62 20 656 10744 144  307 58.8% 0.090\n","-------- EPOCH  4 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:41,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.013. Accuracy: 0.999. Recall: 0.999. Precision: 0.986\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:28,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.017. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:11,  1.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.012. Accuracy: 0.999. Recall: 0.998. Precision: 0.987\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:50,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.013. Accuracy: 0.999. Recall: 0.999. Precision: 0.987\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:52,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 91\n","Runtime for MOT16-02: 89.0 s.\n","Tracking: MOT16-11\n","Tracks found: 80\n","Runtime for MOT16-11: 42.0 s.\n","Runtime for all sequences: 131.1 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 46.5% 66.1% 35.9% 52.2% 96.1%  62 11 38 13 390  8873 103  220 49.6% 0.095\n","MOT16-11 70.6% 77.8% 64.6% 80.2% 96.6%  75 44 24  7 266  1871  34   90 77.0% 0.083\n","OVERALL  55.6% 71.2% 45.6% 61.7% 96.3% 137 55 62 20 656 10744 137  310 58.8% 0.090\n","-------- EPOCH  5 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:35,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.012. Accuracy: 0.999. Recall: 0.999. Precision: 0.988\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:17,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.993\n"]},{"output_type":"stream","name":"stderr","text":["300it [07:53,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.012. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:45,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.009. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:47,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 208\n","Runtime for MOT16-02: 98.4 s.\n","Tracking: MOT16-11\n","Tracks found: 93\n","Runtime for MOT16-11: 42.5 s.\n","Runtime for all sequences: 140.9 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.7% 67.7% 36.8% 52.2% 96.1%  62 12 37 13 390  8873 121  215 49.5% 0.091\n","MOT16-11 71.8% 79.2% 65.7% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.8% 72.7% 46.5% 61.7% 96.3% 137 56 61 20 656 10744 157  305 58.8% 0.087\n","-------- EPOCH  6 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:38,  1.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.015. Accuracy: 0.999. Recall: 0.998. Precision: 0.987\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:24,  1.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:12,  1.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.011. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:48,  1.89s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.013. Accuracy: 0.998. Recall: 0.998. Precision: 0.983\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:49,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 93\n","Runtime for MOT16-02: 89.8 s.\n","Tracking: MOT16-11\n","Tracks found: 80\n","Runtime for MOT16-11: 41.8 s.\n","Runtime for all sequences: 131.6 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.5% 67.5% 36.7% 52.2% 96.1%  62 11 38 13 390  8873 100  220 49.6% 0.095\n","MOT16-11 71.1% 78.3% 65.0% 80.2% 96.6%  75 44 24  7 266  1871  34   90 77.0% 0.083\n","OVERALL  56.4% 72.2% 46.2% 61.7% 96.3% 137 55 62 20 656 10744 134  310 58.8% 0.090\n","-------- EPOCH  7 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:48,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.010. Accuracy: 0.999. Recall: 0.999. Precision: 0.988\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:32,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.010. Accuracy: 0.999. Recall: 0.998. Precision: 0.986\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:10,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.009. Accuracy: 0.999. Recall: 1.000. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:44,  1.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.009. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:49,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 94\n","Runtime for MOT16-02: 89.1 s.\n","Tracking: MOT16-11\n","Tracks found: 82\n","Runtime for MOT16-11: 42.4 s.\n","Runtime for all sequences: 131.5 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.5% 67.5% 36.7% 52.2% 96.1%  62 11 38 13 390  8873  98  218 49.6% 0.095\n","MOT16-11 71.4% 78.7% 65.3% 80.2% 96.6%  75 44 24  7 266  1871  34   90 77.0% 0.083\n","OVERALL  56.5% 72.4% 46.3% 61.7% 96.3% 137 55 62 20 656 10744 132  308 58.8% 0.089\n","-------- EPOCH  8 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:35,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.010. Accuracy: 0.999. Recall: 0.999. Precision: 0.985\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:23,  1.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 1.000. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:11,  1.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.009. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:51,  1.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:56,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 105\n","Runtime for MOT16-02: 90.8 s.\n","Tracking: MOT16-11\n","Tracks found: 91\n","Runtime for MOT16-11: 43.2 s.\n","Runtime for all sequences: 134.0 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.7% 67.7% 36.8% 52.2% 96.1%  62 11 38 13 390  8873 104  219 49.6% 0.095\n","MOT16-11 71.5% 78.8% 65.4% 80.2% 96.6%  75 44 24  7 266  1871  35   90 77.0% 0.083\n","OVERALL  56.6% 72.6% 46.4% 61.7% 96.3% 137 55 62 20 656 10744 139  309 58.8% 0.090\n","-------- EPOCH  9 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:48,  1.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:28,  1.65s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.010. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:02,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:49,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.009. Accuracy: 0.999. Recall: 0.999. Precision: 0.988\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:58,  1.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 108\n","Runtime for MOT16-02: 91.1 s.\n","Tracking: MOT16-11\n","Tracks found: 88\n","Runtime for MOT16-11: 43.0 s.\n","Runtime for all sequences: 134.2 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 48.4% 68.7% 37.4% 52.2% 96.1%  62 11 38 13 390  8873 102  219 49.6% 0.095\n","MOT16-11 71.4% 78.7% 65.3% 80.2% 96.6%  75 44 24  7 266  1871  33   90 77.0% 0.083\n","OVERALL  57.1% 73.1% 46.8% 61.7% 96.3% 137 55 62 20 656 10744 135  309 58.8% 0.090\n","-------- EPOCH 10 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:51,  1.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.008. Accuracy: 0.999. Recall: 1.000. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:36,  1.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:12,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:53,  1.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.009. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:58,  1.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 99\n","Runtime for MOT16-02: 90.6 s.\n","Tracking: MOT16-11\n","Tracks found: 86\n","Runtime for MOT16-11: 42.7 s.\n","Runtime for all sequences: 133.3 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.4% 67.2% 36.5% 52.2% 96.1%  62 11 38 13 390  8873 103  221 49.6% 0.095\n","MOT16-11 71.1% 78.4% 65.0% 80.2% 96.6%  75 44 24  7 266  1871  35   90 77.0% 0.083\n","OVERALL  56.3% 72.1% 46.1% 61.7% 96.3% 137 55 62 20 656 10744 138  311 58.8% 0.090\n","-------- EPOCH 11 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:41,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.008. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:33,  2.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:01,  1.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.010. Accuracy: 0.999. Recall: 0.999. Precision: 0.988\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:51,  1.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.008. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:58,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 103\n","Runtime for MOT16-02: 91.6 s.\n","Tracking: MOT16-11\n","Tracks found: 86\n","Runtime for MOT16-11: 42.8 s.\n","Runtime for all sequences: 134.4 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 47.4% 67.2% 36.5% 52.2% 96.1%  62 11 38 13 390  8873 103  221 49.6% 0.095\n","MOT16-11 70.8% 78.0% 64.8% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.2% 72.0% 46.1% 61.7% 96.3% 137 55 62 20 656 10744 139  311 58.8% 0.090\n","-------- EPOCH 12 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:43,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:23,  1.81s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.009. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:03,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:46,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.989\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:48,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 99\n","Runtime for MOT16-02: 88.1 s.\n","Tracking: MOT16-11\n","Tracks found: 86\n","Runtime for MOT16-11: 40.9 s.\n","Runtime for all sequences: 129.0 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 48.3% 68.5% 37.3% 52.2% 96.1%  62 11 38 13 390  8873 101  221 49.6% 0.095\n","MOT16-11 70.8% 78.0% 64.8% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.7% 72.7% 46.5% 61.7% 96.3% 137 55 62 20 656 10744 137  311 58.8% 0.090\n","-------- EPOCH 13 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:39,  1.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:14,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["300it [07:59,  1.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:36,  1.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:41,  1.60s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 99\n","Runtime for MOT16-02: 87.9 s.\n","Tracking: MOT16-11\n","Tracks found: 86\n","Runtime for MOT16-11: 40.7 s.\n","Runtime for all sequences: 128.6 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 48.3% 68.5% 37.3% 52.2% 96.1%  62 11 38 13 390  8873 101  221 49.6% 0.095\n","MOT16-11 70.8% 78.0% 64.8% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.7% 72.7% 46.5% 61.7% 96.3% 137 55 62 20 656 10744 137  311 58.8% 0.090\n","-------- EPOCH 14 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:40,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:24,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 0.999. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:04,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:39,  1.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.991\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:44,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 99\n","Runtime for MOT16-02: 88.1 s.\n","Tracking: MOT16-11\n","Tracks found: 86\n","Runtime for MOT16-11: 40.8 s.\n","Runtime for all sequences: 128.9 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 48.3% 68.5% 37.2% 52.2% 96.1%  62 11 38 13 390  8873 101  221 49.6% 0.095\n","MOT16-11 70.8% 78.0% 64.8% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.7% 72.7% 46.5% 61.7% 96.3% 137 55 62 20 656 10744 137  311 58.8% 0.090\n","-------- EPOCH 15 --------\n"]},{"output_type":"stream","name":"stderr","text":["100it [02:40,  1.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 100. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.990\n"]},{"output_type":"stream","name":"stderr","text":["200it [05:25,  1.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 200. Loss: 0.008. Accuracy: 0.999. Recall: 1.000. Precision: 0.993\n"]},{"output_type":"stream","name":"stderr","text":["300it [08:12,  1.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 300. Loss: 0.007. Accuracy: 0.999. Recall: 1.000. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["400it [10:51,  1.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Iter 400. Loss: 0.007. Accuracy: 0.999. Recall: 0.999. Precision: 0.992\n"]},{"output_type":"stream","name":"stderr","text":["476it [12:46,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-02\n","Tracks found: 99\n","Runtime for MOT16-02: 87.6 s.\n","Tracking: MOT16-11\n","Tracks found: 87\n","Runtime for MOT16-11: 41.2 s.\n","Runtime for all sequences: 128.8 s.\n","          IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML  FP    FN IDs   FM  MOTA  MOTP\n","MOT16-02 48.3% 68.5% 37.3% 52.2% 96.1%  62 11 38 13 390  8873 101  221 49.6% 0.095\n","MOT16-11 70.8% 78.0% 64.8% 80.2% 96.6%  75 44 24  7 266  1871  36   90 77.0% 0.083\n","OVERALL  56.7% 72.7% 46.5% 61.7% 96.3% 137 55 62 20 656 10744 137  311 58.8% 0.090\n"]}],"source":["best_idf1 = 0.\n","for epoch in range(1, MAX_EPOCHS + 1):\n","    print(f\"-------- EPOCH {epoch:2d} --------\")\n","    train_one_epoch(model = assign_net, data_loader=data_loader, optimizer=optimizer, print_freq=100)\n","    scheduler.step()\n","\n","    if epoch % EVAL_FREQ == 0:\n","        tracker =  MPNTracker(assign_net=assign_net.eval(), obj_detect=None, patience=MAX_PATIENCE)\n","        val_sequences = MOT16Sequences('MOT16-val2', osp.join(root_dir, 'data/MOT16'), vis_threshold=0.)\n","        res = run_tracker(val_sequences, db=train_db, tracker=tracker, output_dir=None)\n","        idf1 = res.loc['OVERALL']['idf1']\n","        if idf1 > best_idf1:\n","            best_idf1 = idf1\n","            torch.save(assign_net.state_dict(), osp.join(root_dir, 'output', 'best_ckpt.pth'))\n","        "]},{"cell_type":"markdown","metadata":{"id":"I5Vbe4g27ot-"},"source":["# Exercise submission\n","\n","After executing the followinc cell the `Colab Notebooks/cv3dst_exercise/output` directory in your Google Drive should contain multiple `MOT16-XY.txt` files.\n","\n","For the final submission you have to process the test sequences and upload the zipped prediction files to our server. See moodle for a guide how to upload the results.\n","\n","Note that this time, you only have to evaluate three sequences!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kJsbbMM7ot_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657277185747,"user_tz":-120,"elapsed":138662,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"b16fc867-0c9d-4b17-c5ee-45a5beebfac5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking: MOT16-01\n","No GT evaluation data available.\n","Tracks found: 78\n","Runtime for MOT16-01: 33.9 s.\n","Writing predictions to: gdrive/My Drive/Colab Notebooks/cv3dst_exercise/output/MOT16-01.txt\n","Tracking: MOT16-08\n","No GT evaluation data available.\n","Tracks found: 131\n","Runtime for MOT16-08: 69.8 s.\n","Writing predictions to: gdrive/My Drive/Colab Notebooks/cv3dst_exercise/output/MOT16-08.txt\n","Tracking: MOT16-12\n","No GT evaluation data available.\n","Tracks found: 136\n","Runtime for MOT16-12: 27.4 s.\n","Writing predictions to: gdrive/My Drive/Colab Notebooks/cv3dst_exercise/output/MOT16-12.txt\n","Runtime for all sequences: 131.1 s.\n"]}],"source":["best_ckpt = torch.load(osp.join(root_dir, 'output', 'best_ckpt.pth'))\n","assign_net.load_state_dict(best_ckpt)\n","\n","tracker =  MPNTracker(assign_net=assign_net.eval(), obj_detect=None, patience=MAX_PATIENCE)\n","test_db = torch.load(osp.join(gnn_root_dir, 'data/preprocessed_data_test_2.pth'))\n","val_sequences = MOT16Sequences('MOT16-test', osp.join(root_dir, 'data/MOT16'), vis_threshold=0.)\n","run_tracker(val_sequences, db=test_db, tracker=tracker, output_dir=osp.join(root_dir, 'output'))"]},{"cell_type":"code","source":["import os\n","os.chdir(\"gdrive/My Drive/Colab Notebooks/cv3dst_exercise/output/\")\n","!zip results.zip MOT16-01.txt MOT16-08.txt MOT16-12.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qCGvPzFDiJj","executionInfo":{"status":"ok","timestamp":1657277185747,"user_tz":-120,"elapsed":13,"user":{"displayName":"Li Zhou","userId":"13700257461378647335"}},"outputId":"7d15b245-ca92-452e-bab1-2f4b2f211272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: MOT16-01.txt (deflated 70%)\n","  adding: MOT16-08.txt (deflated 68%)\n","  adding: MOT16-12.txt (deflated 69%)\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Copy of Copy of CV3DST_Exercise2.ipynb","provenance":[{"file_id":"1eIT4nfc3DGdosPZU2A5SsOSFZrY8JxhL","timestamp":1657077849196}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}